// This file was auto-generated by Fern from our API Definition.

/**
 * This controls the prompt cache retention policy for models that support extended caching (GPT-4.1, GPT-5 series).
 *
 * - `in_memory`: Default behavior, cache retained in GPU memory only
 * - `24h`: Extended caching, keeps cached prefixes active for up to 24 hours by offloading to GPU-local storage
 *
 * Only applies to models: gpt-5.2, gpt-5.1, gpt-5.1-codex, gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1
 *
 * @default undefined (uses API default which is 'in_memory')
 */
export const OpenAiModelPromptCacheRetention = {
    InMemory: "in_memory",
    TwentyFourH: "24h",
} as const;
export type OpenAiModelPromptCacheRetention =
    (typeof OpenAiModelPromptCacheRetention)[keyof typeof OpenAiModelPromptCacheRetention];
