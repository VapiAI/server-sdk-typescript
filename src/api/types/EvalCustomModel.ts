/**
 * This file was auto-generated by Fern from our API Definition.
 */

export interface EvalCustomModel {
    /** This is the provider of the model (`custom-llm`). */
    provider: "custom-llm";
    /** These is the URL we'll use for the OpenAI client's `baseURL`. Ex. https://openrouter.ai/api/v1 */
    url: string;
    /** These are the headers we'll use for the OpenAI client's `headers`. */
    headers?: Record<string, unknown>;
    /** This sets the timeout for the connection to the custom provider without needing to stream any tokens back. Default is 20 seconds. */
    timeoutSeconds?: number;
    /** This is the name of the model. Ex. gpt-4o */
    model: string;
    /** This is the temperature of the model. For LLM-as-a-judge, it's recommended to set it between 0 - 0.3 to avoid hallucinations and ensure the model judges the output correctly based on the instructions. */
    temperature?: number;
    /**
     * This is the max tokens of the model.
     * If your Judge instructions return `true` or `false` takes only 1 token (as per the OpenAI Tokenizer), and therefore is recommended to set it to a low number to force the model to return a short response.
     */
    maxTokens?: number;
}
