/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vapi from "../index.js";

export interface EvalGroqModel {
    /** This is the provider of the model (`groq`). */
    provider: "groq";
    /** This is the name of the model. Ex. gpt-4o */
    model: Vapi.EvalGroqModelModel;
    /** This is the temperature of the model. For LLM-as-a-judge, it's recommended to set it between 0 - 0.3 to avoid hallucinations and ensure the model judges the output correctly based on the instructions. */
    temperature?: number;
    /**
     * This is the max tokens of the model.
     * If your Judge instructions return `true` or `false` takes only 1 token (as per the OpenAI Tokenizer), and therefore is recommended to set it to a low number to force the model to return a short response.
     */
    maxTokens?: number;
}
