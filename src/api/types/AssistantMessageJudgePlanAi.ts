/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as Vapi from "../index.js";

export interface AssistantMessageJudgePlanAi {
    /**
     * This is the model to use for the LLM-as-a-judge.
     * If not provided, will default to the assistant's model.
     *
     * The instructions on how to evaluate the model output with this LLM-Judge must be passed as a system message in the messages array of the model.
     *
     * The Mock conversation can be passed to the LLM-Judge to evaluate using the prompt {{messages}} and will be evaluated as a LiquidJS Variable. To access and judge only the last message, use {{messages[-1]}}
     *
     * The LLM-Judge must respond with "pass" or "fail" and only those two responses are allowed.
     */
    model: AssistantMessageJudgePlanAi.Model;
    /**
     * This is the type of the judge plan.
     * Use 'ai' to evaluate the assistant message content using LLM-as-a-judge.
     * @default 'ai'
     */
    type: "ai";
}

export namespace AssistantMessageJudgePlanAi {
    /**
     * This is the model to use for the LLM-as-a-judge.
     * If not provided, will default to the assistant's model.
     *
     * The instructions on how to evaluate the model output with this LLM-Judge must be passed as a system message in the messages array of the model.
     *
     * The Mock conversation can be passed to the LLM-Judge to evaluate using the prompt {{messages}} and will be evaluated as a LiquidJS Variable. To access and judge only the last message, use {{messages[-1]}}
     *
     * The LLM-Judge must respond with "pass" or "fail" and only those two responses are allowed.
     */
    export type Model = Vapi.EvalOpenAiModel | Vapi.EvalAnthropicModel | Vapi.EvalGoogleModel | Vapi.EvalCustomModel;
}
